# üìä Evaluation Methodology

> H∆∞·ªõng d·∫´n ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng h·ªá th·ªëng Study Planner s·ª≠ d·ª•ng F1 Score

---

## 1. T·ªïng quan Ph∆∞∆°ng ph√°p ƒê√°nh gi√°

### V·∫•n ƒë·ªÅ: L√†m sao bi·∫øt AI t·∫°o plan t·ªët?

V·ªõi LLM applications, kh√¥ng c√≥ "ground truth" r√µ r√†ng nh∆∞ classification tasks. Ch√∫ng ta s·ª≠ d·ª•ng **Implicit Feedback** (feedback ng·∫ßm) t·ª´ h√†nh vi user.

### Ngu·ªìn c·∫£m h·ª©ng

> "User dissatisfaction can be inferred from behavioral patterns, particularly query reformulation."
> ‚Äî Bing Research, 2023

---

## 2. Implicit Signals (T√≠n hi·ªáu Ng·∫ßm)

### Positive Signals (Plan t·ªët) ‚úÖ

| Signal | Weight | Interpretation |
|--------|--------|----------------|
| **Save** | 1.0 | User h√†i l√≤ng, mu·ªën gi·ªØ l·∫°i |
| **Share** | 1.2 | User t·ª± tin share cho ng∆∞·ªùi kh√°c |
| **Time spent > 60s** | 0.5 | User ƒë·ªçc k·ªπ plan |
| **Scroll to bottom** | 0.3 | User xem h·∫øt n·ªôi dung |
| **Return visit** | 0.8 | User quay l·∫°i xem plan |

### Negative Signals (Plan kh√¥ng t·ªët) ‚ùå

| Signal | Weight | Interpretation |
|--------|--------|----------------|
| **Regenerate** | -1.0 | User kh√¥ng h√†i l√≤ng |
| **Regenerate x2** | -1.5 | V·∫´n kh√¥ng h√†i l√≤ng sau l·∫ßn 2 |
| **Regenerate x3+** | -2.0 | H·ªá th·ªëng fail ho√†n to√†n |
| **Abandon < 10s** | -0.8 | User b·ªè ƒëi ngay |
| **Close tab < 30s** | -0.5 | Kh√¥ng quan t√¢m ƒë·∫øn plan |

---

## 3. T√≠nh F1 Score

### ƒê·ªãnh nghƒ©a cho Context n√†y

```
True Positive (TP)  = Plan ƒë∆∞·ª£c Save (user h√†i l√≤ng)
False Positive (FP) = Plan ƒë∆∞·ª£c generate nh∆∞ng b·ªã Regenerate (AI nghƒ© ok, user nghƒ© kh√¥ng)
False Negative (FN) = Plan hay nh∆∞ng user kh√¥ng Save (kh√≥ ƒëo, ∆∞·ªõc l∆∞·ª£ng t·ª´ sampling)
True Negative (TN)  = Kh√¥ng √°p d·ª•ng
```

### C√¥ng th·ª©c

```
Precision = TP / (TP + FP)
          = Saved / (Saved + Regenerated)
          = "Trong s·ªë plans AI t·∫°o ra, bao nhi√™u % user th·ª±c s·ª± d√πng?"

Recall = TP / (TP + FN)
       = "Trong s·ªë plans user c·∫ßn, bao nhi√™u % AI t·∫°o ƒë√∫ng?"
       ‚âà ∆Ø·ªõc l∆∞·ª£ng t·ª´ LLM-as-a-Judge

F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

### V√≠ d·ª• T√≠nh to√°n

```python
# Tu·∫ßn 1 data
total_generations = 100
saved = 65
regenerated = 30
abandoned = 5

# Precision
precision = saved / (saved + regenerated)
# = 65 / 95 = 0.684 (68.4%)

# Recall (t·ª´ LLM Judge tr√™n 5% sample)
# Judge ƒë√°nh gi√° 5 plans: 4 "good", 1 "poor but user saved anyway"
recall = 4 / 5 = 0.80

# F1 Score
f1 = 2 * (0.684 * 0.80) / (0.684 + 0.80)
# = 0.736 (73.6%)
```

---

## 4. LLM-as-a-Judge Pipeline

### Khi n√†o ch·∫°y Judge?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ All Generations ‚îÇ
‚îÇ    (100%)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº Random Sample
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5% Sampled for  ‚îÇ
‚îÇ Quality Review  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº GPT-4o Judge
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Scored Plans    ‚îÇ
‚îÇ (5 criteria)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ F1 Calculation  ‚îÇ
‚îÇ + Feedback Loop ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Sampling Strategy

```python
def should_sample_for_judge(plan_metadata: dict) -> bool:
    """
    Quy·∫øt ƒë·ªãnh c√≥ g·ª≠i plan n√†y cho Judge kh√¥ng.
    """
    import random
    
    # Base rate: 5%
    if random.random() > 0.05:
        return False
    
    # ∆Øu ti√™n sample c√°c cases th√∫ v·ªã
    priority_cases = [
        plan_metadata.get("regenerate_count", 0) >= 2,  # Nhi·ªÅu regenerate
        plan_metadata.get("router_decision") == "hard", # Complex inputs
        plan_metadata.get("model_used") == "gpt-4o",    # Expensive model
    ]
    
    if any(priority_cases):
        return random.random() < 0.20  # 20% cho priority cases
    
    return True
```

### Judge Evaluation Code

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Literal

class JudgeScore(BaseModel):
    completeness: int = Field(ge=1, le=5)
    feasibility: int = Field(ge=1, le=5)
    pedagogical_soundness: int = Field(ge=1, le=5)
    clarity: int = Field(ge=1, le=5)
    personalization: int = Field(ge=1, le=5)
    overall_score: float = Field(ge=1.0, le=5.0)
    verdict: Literal["good", "acceptable", "poor"]
    feedback: str
    
async def evaluate_plan(
    original_input: str,
    generated_plan: dict,
    user_action: str
) -> JudgeScore:
    """
    ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng plan b·∫±ng GPT-4o.
    """
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    structured_llm = llm.with_structured_output(JudgeScore)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", JUDGE_SYSTEM_PROMPT),
        ("human", JUDGE_USER_TEMPLATE)
    ])
    
    chain = prompt | structured_llm
    
    result = await chain.ainvoke({
        "original_input": original_input,
        "generated_plan": json.dumps(generated_plan, ensure_ascii=False),
        "user_action": user_action
    })
    
    return result
```

---

## 5. Metrics Dashboard

### Daily Metrics

```python
@dataclass
class DailyMetrics:
    date: str
    total_generations: int
    unique_users: int
    
    # Core metrics
    save_count: int
    regenerate_count: int
    abandon_count: int
    
    # Calculated
    @property
    def precision(self) -> float:
        denom = self.save_count + self.regenerate_count
        return self.save_count / denom if denom > 0 else 0
    
    # Cost metrics
    gemini_flash_calls: int
    gpt4o_calls: int
    total_cost_usd: float
    
    # Performance
    avg_response_time_ms: int
    p95_response_time_ms: int
    
    # Router accuracy
    router_easy_correct: int  # Easy ‚Üí Flash ‚Üí Saved
    router_hard_correct: int  # Hard ‚Üí GPT4 ‚Üí Saved
    router_easy_wrong: int    # Easy ‚Üí Flash ‚Üí Regenerated
    router_hard_wrong: int    # Hard ‚Üí GPT4 ‚Üí Regenerated
```

### Weekly Report Template

```markdown
## Weekly Quality Report - Week 3, 2026

### üìä Key Metrics
| Metric | This Week | Last Week | Change |
|--------|-----------|-----------|--------|
| F1 Score | 0.74 | 0.71 | +4.2% ‚úÖ |
| Precision | 0.68 | 0.65 | +4.6% ‚úÖ |
| Recall (Judge) | 0.82 | 0.80 | +2.5% ‚úÖ |

### üîÄ Router Performance
- Easy ‚Üí Correct: 85%
- Hard ‚Üí Correct: 78%
- Misrouted: 12%

### üí∞ Cost Analysis
- Total API cost: $45.20
- Cost per generation: $0.032
- Gemini Flash: 89% of calls, 23% of cost
- GPT-4o: 11% of calls, 77% of cost

### üêõ Top Issues (from Regenerations)
1. "Plan qu√° d√†y ƒë·∫∑c" - 23 occurrences
2. "Thi·∫øu breaks" - 18 occurrences
3. "Th·ªùi gian kh√¥ng h·ª£p l√Ω" - 12 occurrences

### üéØ Action Items
- [ ] Adjust Planner prompt to reduce density
- [ ] Add explicit break scheduling rule
- [ ] Review time estimation logic
```

---

## 6. Feedback Loop (T·ª± c·∫£i thi·ªán)

### Automatic Prompt Tuning

```python
async def analyze_failures_and_suggest_improvements():
    """
    Ph√¢n t√≠ch c√°c plans b·ªã regenerate nhi·ªÅu,
    ƒë·ªÅ xu·∫•t c·∫£i thi·ªán prompt.
    """
    # 1. L·∫•y 20 plans b·ªã regenerate >= 2 l·∫ßn
    failed_plans = await db.get_failed_plans(limit=20)
    
    # 2. T√¨m pattern chung
    analysis_prompt = """
    Analyze these failed study plans and identify common issues:
    {failed_plans}
    
    Output:
    - Top 3 recurring problems
    - Suggested prompt modifications
    - Example improvements
    """
    
    # 3. Update prompt suggestions (con ng∆∞·ªùi review)
    suggestions = await llm.ainvoke(analysis_prompt)
    await notify_team(suggestions)
```

### A/B Testing Framework

```python
class PromptExperiment:
    """
    Test 2 versions of prompt ƒë·ªÉ xem version n√†o c√≥ F1 cao h∆°n.
    """
    def __init__(self, name: str, control: str, treatment: str):
        self.name = name
        self.control_prompt = control
        self.treatment_prompt = treatment
        self.traffic_split = 0.5  # 50/50
        
    async def get_prompt(self, session_id: str) -> str:
        # Consistent assignment based on session
        is_treatment = hash(session_id) % 100 < (self.traffic_split * 100)
        return self.treatment_prompt if is_treatment else self.control_prompt
    
    async def analyze_results(self, min_samples: int = 100) -> dict:
        control_f1 = await self._calculate_f1("control")
        treatment_f1 = await self._calculate_f1("treatment")
        
        return {
            "control_f1": control_f1,
            "treatment_f1": treatment_f1,
            "winner": "treatment" if treatment_f1 > control_f1 else "control",
            "confidence": self._calculate_significance()
        }
```

---

## 7. LangSmith Integration

### Logging Code

```python
import os
from langsmith import Client
from langchain.callbacks import LangChainTracer

# Setup
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "student-planner-prod"

client = Client()

# Create dataset for evaluation
def log_for_evaluation(
    run_id: str,
    input_data: dict,
    output_data: dict,
    user_action: str
):
    """
    Log data point cho F1 calculation.
    """
    client.create_example(
        dataset_name="planner-quality-eval",
        inputs=input_data,
        outputs={
            **output_data,
            "user_action": user_action,
            "is_positive": user_action == "save"
        }
    )
```

### Running Evaluation

```python
from langsmith.evaluation import evaluate

def quality_evaluator(run, example):
    """
    Custom evaluator cho LangSmith.
    """
    prediction = run.outputs.get("json_plan", {})
    reference_action = example.outputs.get("user_action")
    
    # Simple heuristic
    if reference_action == "save":
        return {"score": 1.0, "label": "positive"}
    elif reference_action == "regenerate":
        return {"score": 0.0, "label": "negative"}
    else:
        return {"score": 0.5, "label": "neutral"}

# Run evaluation
results = evaluate(
    lambda x: generate_plan(x["input"]),
    data="planner-quality-eval",
    evaluators=[quality_evaluator],
    experiment_prefix="prompt-v2"
)

print(f"Overall F1: {results.summary['f1_score']}")
```

---

## 8. Targets & Alerts

### Quality Thresholds

```yaml
# alerts.yaml
metrics:
  f1_score:
    warning: 0.65
    critical: 0.55
    target: 0.75
    
  precision:
    warning: 0.60
    critical: 0.50
    target: 0.70
    
  avg_response_time_ms:
    warning: 8000
    critical: 15000
    target: 5000
    
  cost_per_generation_usd:
    warning: 0.08
    critical: 0.15
    target: 0.05

alerts:
  - name: "F1 Score Drop"
    condition: "f1_score < warning for 1 hour"
    action: "slack_notify"
    
  - name: "High Regenerate Rate"
    condition: "regenerate_count / total > 0.4 for 30 min"
    action: "slack_notify + pause_traffic"
```

---

## 9. Checklist Tr∆∞·ªõc Production

- [ ] LangSmith project created v√† configured
- [ ] Sampling rate set (5% default)
- [ ] Judge prompt tested v√† validated
- [ ] Daily metrics dashboard deployed
- [ ] Alerting rules configured
- [ ] Feedback table in Firestore created
- [ ] First 50 manual labels for baseline
- [ ] A/B testing framework ready

---

*Document version: 1.0*  
*Last updated: 2026-01-18*
